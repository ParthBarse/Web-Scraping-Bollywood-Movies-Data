# -*- coding: utf-8 -*-
"""web_scrapping_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ecv-fdtpKimyNn6dkQgZHMS9KoNG6NMz

# Importing Required Libraries
"""

from bs4 import BeautifulSoup as bs
import html5lib
import requests
import csv
import json

"""# Collecting Data from Web"""

url = "https://en.wikipedia.org/wiki/Lists_of_Bollywood_films"
r = requests.get(url)
soup = bs(r.content)

np_lnks = soup.find_all("li")

np_lnks = np_lnks[13:115]

# np_lnks_2021_2023 = np_lnks[0:3]
# np_lnks_2016_2020 = np_lnks[3:8]
# np_lnks_2011_2015 = np_lnks[8:13]
# np_lnks_2006_2010 = np_lnks[13:18]
# np_lnks_2001_2005 = np_lnks[18:23]

# np_lnks_old_1 = np_lnks[23:43]
# np_lnks_old_2 = np_lnks[43:63]
# np_lnks_old_3 = np_lnks[63:83]

# len(np_lnks)

# np_lnks_old_4 = np_lnks[90:115]

# np_lnks_old_4

"""# Defining Modules"""

def get_content_value(row_data):
    if row_data.find("li"):
      return [li.get_text(" ", strip = True).replace("\xa0", " ") for li in row_data.find_all("li")]
    
    
    elif row_data.find("br"):
      return [text for text in row_data.stripped_strings]

    else:
        return row_data.get_text(" ", strip = True).replace("\xa0", " ")

def clean_tags(soup):
  for tag in soup.find_all(["sup", "span"]):
    tag.decompose()


def get_info_box(url):
    
    r = requests.get(url)
    soup = bs(r.content)
    info_box = soup.find(class_ = "infobox vevent")
    info_rows = info_box.find_all("tr")

    clean_tags(soup)

    movie_info = {}
    for index, row in enumerate(info_rows):
        if index == 0:
            movie_info["title"] = row.find("th").get_text(" ", strip = True)

        else:
          header = row.find("th")
          if header:
            content_key = row.find("th").get_text(" ", strip = True)
            content_value = get_content_value(row.find("td"))
            movie_info[content_key] = content_value
        
    return movie_info

"""# Ignore"""

# # For 2021_2023
# base_path = "https://en.wikipedia.org"


# for s_lnk_2021_2023 in np_lnks_2021_2023:
    
#     title_text = s_lnk_2021_2023.a["title"]
#     path_2021_2023 = s_lnk_2021_2023.a["href"]
    
#     full_path_2021_2023 = base_path + path_2021_2023
    
#     url_2021_2023 = full_path_2021_2023
#     r = requests.get(url_2021_2023)
#     soup_2021_2023 = bs(r.content)
    
#     movies_2021_2023 = soup_2021_2023.select(".wikitable i")
    
#     base_path = "https://en.wikipedia.org"
    
#     movie_info_list_2021_2023 = []
    
#     for movie_2021_2023 in movies_2021_2023:
        
#         try:
#             relati_path_2021_2023 = movie_2021_2023.a["href"]
#             title_2021_2023 = movie_2021_2023.a["title"]
            
#             full_path_2021_2023_2nd = base_path + relati_path_2021_2023
            
#             movie_info_list_2021_2023.append(get_info_box(full_path_2021_2023_2nd))
            
#             print("Added >> ", title_2021_2023)
#             print("Added >> ", relati_path_2021_2023)
#             print()
            
#         except Exception as e:
#             print("Err : ", movie_2021_2023.get_text())
#             print("Err : ", e)
#             print()

# # movie_info_list_2021_2023
# len(movie_info_list_2021_2023)

# # For 2016_2020
# base_path = "https://en.wikipedia.org"


# for s_lnk_2016_2020 in np_lnks_2016_2020:
    
#     title_text = s_lnk_2016_2020.a["title"]
#     path_2016_2020 = s_lnk_2016_2020.a["href"]
    
#     full_path_2016_2020 = base_path + path_2016_2020
    
#     url_2016_2020 = full_path_2016_2020
#     r = requests.get(url_2016_2020)
#     soup_2016_2020 = bs(r.content)
    
#     movies_2016_2020 = soup_2016_2020.select(".wikitable i")
    
#     base_path = "https://en.wikipedia.org"
    
#     movie_info_list_2016_2020 = []
    
#     for movie_2016_2020 in movies_2016_2020:
        
#         try:
#             relati_path_2016_2020 = movie_2016_2020.a["href"]
#             title_2016_2020 = movie_2016_2020.a["title"]
            
#             full_path_2016_2020_2nd = base_path + relati_path_2016_2020
            
#             movie_info_list_2016_2020.append(get_info_box(full_path_2016_2020_2nd))
            
#             print("Added >> ", title_2016_2020)
#             print("Added >> ", relati_path_2016_2020)
#             print()
            
#         except Exception as e:
#             print("Err : ", movie_2016_2020.get_text())
#             print("Err : ", e)
#             print()

# # movie_info_list_2016_2020
# len(movie_info_list_2016_2020)

# # For 2011_2015
# base_path = "https://en.wikipedia.org"


# for s_lnk_2011_2015 in np_lnks_2011_2015:
    
#     title_text = s_lnk_2011_2015.a["title"]
#     path_2011_2015 = s_lnk_2011_2015.a["href"]
    
#     full_path_2011_2015 = base_path + path_2011_2015
    
#     url_2011_2015 = full_path_2011_2015
#     r = requests.get(url_2011_2015)
#     soup_2011_2015 = bs(r.content)
    
#     movies_2011_2015 = soup_2011_2015.select(".wikitable i")
    
#     base_path = "https://en.wikipedia.org"
    
#     movie_info_list_2011_2015 = []
    
#     for movie_2011_2015 in movies_2011_2015:
        
#         try:
#             relati_path_2011_2015 = movie_2011_2015.a["href"]
#             title_2011_2015 = movie_2011_2015.a["title"]
            
#             full_path_2011_2015_2nd = base_path + relati_path_2011_2015
            
#             movie_info_list_2011_2015.append(get_info_box(full_path_2011_2015_2nd))
            
#             print("Added >> ", title_2011_2015)
#             print("Added >> ", relati_path_2011_2015)
#             print()
            
#         except Exception as e:
#             print("Err : ", movie_2011_2015.get_text())
#             print("Err : ", e)
#             print()

# # movie_info_list_2011_2015
# len(movie_info_list_2011_2015)

# # For 2006_2010
# base_path = "https://en.wikipedia.org"


# for s_lnk_2006_2010 in np_lnks_2006_2010:
    
#     title_text = s_lnk_2006_2010.a["title"]
#     path_2006_2010 = s_lnk_2006_2010.a["href"]
    
#     full_path_2006_2010 = base_path + path_2006_2010
    
#     url_2006_2010 = full_path_2006_2010
#     r = requests.get(url_2006_2010)
#     soup_2006_2010 = bs(r.content)
    
#     movies_2006_2010 = soup_2006_2010.select(".wikitable i")
    
#     base_path = "https://en.wikipedia.org"
    
#     movie_info_list_2006_2010 = []
    
#     for movie_2006_2010 in movies_2006_2010:
        
#         try:
#             relati_path_2006_2010 = movie_2006_2010.a["href"]
#             title_2006_2010 = movie_2006_2010.a["title"]
            
#             full_path_2006_2010_2nd = base_path + relati_path_2006_2010
            
#             movie_info_list_2006_2010.append(get_info_box(full_path_2006_2010_2nd))
            
#             print("Added >> ", title_2006_2010)
#             print("Added >> ", relati_path_2006_2010)
#             print()
            
#         except Exception as e:
#             print("Err : ", movie_2006_2010.get_text())
#             print("Err : ", e)
#             print()

# # movie_info_list_2006_2010
# len(movie_info_list_2006_2010)

# #@title Default title text
# # For 2001_2005
# base_path = "https://en.wikipedia.org"


# for s_lnk_2001_2005 in np_lnks_2001_2005:
    
#     title_text = s_lnk_2001_2005.a["title"]
#     path_2001_2005 = s_lnk_2001_2005.a["href"]
    
#     full_path_2001_2005 = base_path + path_2001_2005
    
#     url_2001_2005 = full_path_2001_2005
#     r = requests.get(url_2001_2005)
#     soup_2001_2005 = bs(r.content)
    
#     movies_2001_2005 = soup_2001_2005.select(".wikitable i")
    
#     base_path = "https://en.wikipedia.org"
    
#     movie_info_list_2001_2005 = []
    
#     for movie_2001_2005 in movies_2001_2005:
        
#         try:
#             relati_path_2001_2005 = movie_2001_2005.a["href"]
#             title_2001_2005 = movie_2001_2005.a["title"]
            
#             full_path_2001_2005_2nd = base_path + relati_path_2001_2005
            
#             movie_info_list_2001_2005.append(get_info_box(full_path_2001_2005_2nd))
            
#             print("Added >> ", title_2001_2005)
#             print("Added >> ", relati_path_2001_2005)
#             print()
            
#         except Exception as e:
#             print("Err : ", movie_2001_2005.get_text())
#             print("Err : ", e)
#             print()

# # movie_info_list_2001_2005
# len(movie_info_list_2001_2005)

# # For old_1
# base_path = "https://en.wikipedia.org"

# movie_info_list_old_1 = []


# for s_lnk_old_1 in np_lnks_old_1:
    
#     title_text = s_lnk_old_1.a["title"]
#     path_old_1 = s_lnk_old_1.a["href"]
    
#     full_path_old_1 = base_path + path_old_1
    
#     url_old_1 = full_path_old_1
#     r = requests.get(url_old_1)
#     soup_old_1 = bs(r.content)
    
#     movies_old_1 = soup_old_1.select(".wikitable i")
    
#     base_path = "https://en.wikipedia.org"
    
#     for movie_old_1 in movies_old_1:
        
#         try:
#             relati_path_old_1 = movie_old_1.a["href"]
#             title_old_1 = movie_old_1.a["title"]
            
#             full_path_old_1_2nd = base_path + relati_path_old_1
            
#             movie_info_list_old_1.append(get_info_box(full_path_old_1_2nd))
            
#             print("Added >> ", title_old_1)
#             print("Added >> ", relati_path_old_1)
#             print()
            
#         except Exception as e:
#             print("Err : ", movie_old_1.get_text())
#             print("Err : ", e)
#             print()

# # movie_info_list_old_1
# len(movie_info_list_old_1)

# # For old_2
# base_path = "https://en.wikipedia.org"

# movie_info_list_old_2 = []


# for s_lnk_old_2 in np_lnks_old_2:
    
#     title_text = s_lnk_old_2.a["title"]
#     path_old_2 = s_lnk_old_2.a["href"]
    
#     full_path_old_2 = base_path + path_old_2
    
#     url_old_2 = full_path_old_2
#     r = requests.get(url_old_2)
#     soup_old_2 = bs(r.content)
    
#     movies_old_2 = soup_old_2.select(".wikitable i")
    
#     base_path = "https://en.wikipedia.org"
    
#     for movie_old_2 in movies_old_2:
        
#         try:
#             relati_path_old_2 = movie_old_2.a["href"]
#             title_old_2 = movie_old_2.a["title"]
            
#             full_path_old_2_2nd = base_path + relati_path_old_2
            
#             movie_info_list_old_2.append(get_info_box(full_path_old_2_2nd))
            
#             print("Added >> ", title_old_2)
#             print("Added >> ", relati_path_old_2)
#             print()
            
#         except Exception as e:
#             print("Err : ", movie_old_2.get_text())
#             print("Err : ", e)
#             print()

# # movie_info_list_old_2
# len(movie_info_list_old_2)

# # For old_3
# base_path = "https://en.wikipedia.org"

# movie_info_list_old_3 = []


# for s_lnk_old_3 in np_lnks_old_3:
    
#     title_text = s_lnk_old_3.a["title"]
#     path_old_3 = s_lnk_old_3.a["href"]
    
#     full_path_old_3 = base_path + path_old_3
    
#     url_old_3 = full_path_old_3
#     r = requests.get(url_old_3)
#     soup_old_3 = bs(r.content)
    
#     movies_old_3 = soup_old_3.select(".wikitable i")
    
#     base_path = "https://en.wikipedia.org"
    
#     for movie_old_3 in movies_old_3:
        
#         try:
#             relati_path_old_3 = movie_old_3.a["href"]
#             title_old_3 = movie_old_3.a["title"]
            
#             full_path_old_3_2nd = base_path + relati_path_old_3
            
#             movie_info_list_old_3.append(get_info_box(full_path_old_3_2nd))
            
#             print("Added >> ", title_old_3)
#             print("Added >> ", relati_path_old_3)
#             print()
            
#         except Exception as e:
#             print("Err : ", movie_old_3.get_text())
#             print("Err : ", e)
#             print()

# # movie_info_list_old_3
# len(movie_info_list_old_3)

# raw_data = [*movie_info_list_2021_2023, *movie_info_list_2016_2020, *movie_info_list_2011_2015, *movie_info_list_2006_2010, *movie_info_list_2001_2005, *movie_info_list_old_1, *movie_info_list_old_2, *movie_info_list_old_3]

# len(raw_data)

"""# Collecting Information about each Movie"""

# For All
base_path = "https://en.wikipedia.org"

movie_info_list = []
skipped_movies_list = []


for s_lnk in np_lnks:
    
    title_text = s_lnk.a["title"]
    path = s_lnk.a["href"]
    
    full_path = base_path + path
    
    url = full_path
    r = requests.get(url)
    soup = bs(r.content)
    
    movies = soup.select(".wikitable i")
    
    base_path = "https://en.wikipedia.org"
    
    for movie in movies:
        
        try:
            relati_path = movie.a["href"]
            title = movie.a["title"]
            
            full_path_2nd = base_path + relati_path
            
            movie_info_list.append(get_info_box(full_path_2nd))
            
            print("Added >> ", title)
            print("Added >> ", relati_path)
            print("Count >> ", len(movie_info_list))
            print()
            
        except Exception as e:
          skipped_movies_list.append(movie.get_text())
          print("Err : ", movie.get_text())
          print("Err : ", e)
          print("Skipped ", len(skipped_movies_list))
          print()

len(movie_info_list)
# len(skipped_movies_list)

# movie_info_list[5]
skipped_movies_list

"""# Saving and Reloading Data """

# field_names = ['title', 'Directed by', 'Written by', 'Story by', 'Based on', 'Starring', 'Cinematography', 'Edited by', 'Music by', 'Production companies', 'Production company', 'Produced by', 'Screenplay by', "Animation by", "Color process",'Original language', 'Setting', 'Genre', 'Date premiered', 'Characters', 'Narrated by', 'Distributed by', 'Release date', 'Release dates', 'Running time', 'Country', 'Countries', 'Language', 'Languages', 'Budget', 'Box office']

# with open('scrapped_raw_data.csv', 'w') as csvfile:
#     writer = csv.DictWriter(csvfile, fieldnames = field_names)
#     writer.writeheader()
#     writer.writerows(movie_info_list)

def save_data(title, data):
  with open(title, 'w', encoding='utf-8') as f:
    json.dump(data, f, ensure_ascii=False, indent=2)

def load_data(title):
  with open(title, encoding="utf-8") as f:
    return json.load(f)

save_data("bollywood_movies_data.json", movie_info_list)

movie_info_list = load_data("bollywood_movies_data.json")

"""# Cleaning Data - 

1.   Clean up references [1]
2.   Convert running time into integer
3.   Converting Date into datetime object
4.   Split up the long strings
5.   Convert Budget and Box Office to numbers


"""

